{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.datasets import EllipticBitcoinDataset\n",
    "from torch_geometric.utils import k_hop_subgraph\n",
    "from sklearn.metrics import classification_report\n",
    "import requests\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/Transaction'\n",
    "base_url = 'http://localhost:5004'\n",
    "\n",
    "node_url = f\"{base_url}/api/nodes\"\n",
    "edge_url = f\"{base_url}/api/edges\"\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "batch_size = 10_000\n",
    "transactions_raw_file = f'{data_dir}/transactions_raw.pkl'\n",
    "transactions_file = f'{data_dir}/transactions.pkl'\n",
    "\n",
    "dataset_lib = EllipticBitcoinDataset(root=f'{data_dir}/EllipticBitcoin', transform=None)\n",
    "data_lib = dataset_lib[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data(url, headers, batch_size, page):\n",
    "    params = {\"page_size\": batch_size, \"page\": page}\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def process_nodes(existing_nodes, new_nodes):\n",
    "    node_id_map = {node['id']: idx for idx, node in enumerate(existing_nodes)}\n",
    "    start_idx = len(existing_nodes)\n",
    "    \n",
    "    for node in new_nodes:\n",
    "        if node['id'] not in node_id_map:\n",
    "            node_id_map[node['id']] = start_idx\n",
    "            existing_nodes.append(node)\n",
    "            start_idx += 1\n",
    "\n",
    "\n",
    "def process_edges(existing_edges, new_edges, existing_nodes):\n",
    "    node_id_map = {node['id']: idx for idx, node in enumerate(existing_nodes)}\n",
    "\n",
    "    for edge in new_edges:\n",
    "        if edge['src'] in node_id_map and edge['dst'] in node_id_map:\n",
    "            existing_edges.append(edge)\n",
    "\n",
    "\n",
    "def save_data(data, file_path):\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "\n",
    "def load_data(file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    return None\n",
    "\n",
    "\n",
    "def fetch_and_process_graph_data(file_path):\n",
    "    existing_nodes, existing_edges, node_page, edge_page = load_data(file_path)\n",
    "\n",
    "    if existing_nodes is None:\n",
    "        existing_nodes = []\n",
    "    if existing_edges is None:\n",
    "        existing_edges = []\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            node_data = fetch_data(node_url, headers, batch_size, node_page)\n",
    "            new_nodes = node_data['results']\n",
    "            \n",
    "            if not new_nodes:\n",
    "                break\n",
    "            \n",
    "            process_nodes(existing_nodes, new_nodes)\n",
    "            start_idx = (node_page - 1) * batch_size\n",
    "            end_idx = start_idx + len(new_nodes) - 1\n",
    "            print(f\"Nodes {start_idx}-{end_idx} retrieved.\")\n",
    "            node_page += 1\n",
    "            save_data((existing_nodes, existing_edges, node_page, edge_page), file_path)\n",
    "\n",
    "        while True:\n",
    "            edge_data = fetch_data(edge_url, headers, batch_size, edge_page)\n",
    "            new_edges = edge_data['results']\n",
    "            \n",
    "            if not new_edges:\n",
    "                break\n",
    "            \n",
    "            process_edges(existing_edges, new_edges, existing_nodes)\n",
    "            start_idx = (edge_page - 1) * batch_size\n",
    "            end_idx = start_idx + len(new_edges) - 1\n",
    "            print(f\"Edges {start_idx}-{end_idx} retrieved.\")\n",
    "            edge_page += 1\n",
    "            save_data((existing_nodes, existing_edges, node_page, edge_page), file_path)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        save_data((existing_nodes, existing_edges, node_page, edge_page), file_path)\n",
    "        raise\n",
    "\n",
    "    return existing_nodes, existing_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes, edges = fetch_and_process_graph_data(transactions_raw_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_object(nodes, edges, train_mask=None, test_mask=None):\n",
    "    sorted_nodes = sorted(nodes, key=lambda node: node['orig_id'])\n",
    "    sorted_edges = sorted(edges, key=lambda edge: edge['orig_id'])\n",
    "\n",
    "    node_features = []\n",
    "    node_labels = []\n",
    "    node_id_map = {}\n",
    "\n",
    "    for idx, node in enumerate(sorted_nodes):\n",
    "        node_id_map[node['id']] = idx\n",
    "        node_features.append(node['x'])\n",
    "        node_labels.append(node['y'])\n",
    "\n",
    "    node_features = torch.tensor(node_features, dtype=torch.float)\n",
    "    node_labels = torch.tensor(node_labels, dtype=torch.long)\n",
    "\n",
    "    edge_index = []\n",
    "    for edge in sorted_edges:\n",
    "        if edge['src'] in node_id_map and edge['dst'] in node_id_map:\n",
    "            src = node_id_map[edge['src']]\n",
    "            dst = node_id_map[edge['dst']]\n",
    "            edge_index.append([src, dst])\n",
    "\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "\n",
    "    return Data(x=node_features, edge_index=edge_index, y=node_labels, train_mask=train_mask, test_mask=test_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = create_data_object(nodes, edges, data_lib.train_mask.clone(), data_lib.test_mask.clone())\n",
    "pickle.dump(data, open(transactions_file, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "        self.num_hops = sum(1 for layer in self.children() if isinstance(layer, GCNConv)) + 1\n",
    "        self.optimizer = None\n",
    "        self.loss_fn = None\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "    def compile(self, optimizer, loss_fn, class_weights=None):\n",
    "        self.optimizer = optimizer\n",
    "        if class_weights is not None:\n",
    "            class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(next(self.parameters()).device)\n",
    "            self.loss_fn = loss_fn(weight=class_weights_tensor)\n",
    "        else:\n",
    "            self.loss_fn = loss_fn()\n",
    "\n",
    "    def fit(self, data, epochs=1000):\n",
    "        for epoch in range(epochs):\n",
    "            self.train()\n",
    "            self.optimizer.zero_grad()\n",
    "            out = self(data)\n",
    "            loss = self.loss_fn(out[data.train_mask], data.y[data.train_mask])\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                acc = self.evaluate(data)\n",
    "                print(f'Epoch {epoch}, Loss: {loss:.4f}, Test Accuracy: {acc:.4f}')\n",
    "\n",
    "    def predict(self, data, node_idx=None):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            if node_idx is not None:\n",
    "                subset, edge_index, mapping, _ = k_hop_subgraph(node_idx, self.num_hops, data.edge_index, relabel_nodes=True)\n",
    "                sub_data = Data(x=data.x[subset], edge_index=edge_index)\n",
    "            else:\n",
    "                sub_data = data\n",
    "            out = self(sub_data)\n",
    "            probabilities = F.softmax(out, dim=1)\n",
    "            predictions = probabilities.argmax(dim=1)\n",
    "            if node_idx is not None:\n",
    "                return predictions[mapping.item()], probabilities[mapping.item()]\n",
    "            else:\n",
    "                return predictions, probabilities\n",
    "\n",
    "    def evaluate(self, data):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            out = self(data)\n",
    "            pred = out.argmax(dim=1)\n",
    "            correct = pred[data.test_mask] == data.y[data.test_mask]\n",
    "            acc = int(correct.sum()) / int(data.test_mask.sum())\n",
    "            return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCN(in_channels=data.x.shape[1], hidden_channels=100, out_channels=2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = [0.3, 0.7]\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=5e-4)\n",
    "model.compile(optimizer, torch.nn.CrossEntropyLoss, class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.3002, Test Accuracy: 0.5040\n"
     ]
    }
   ],
   "source": [
    "model.fit(data, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'gcn_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0, device='cuda:0'), tensor([0.6453, 0.3547], device='cuda:0'))"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
